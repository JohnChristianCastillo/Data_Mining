{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last weekend, I asked GPT4 from OpenAI to implement an association rule mining algorithm in Python. GPT-4 responded with the following:\n",
    "`Association rule mining is a technique used in data mining to discover relationships between variables in large datasets. One popular algorithm for association rule mining is the Apriori algorithm. The following is a simple implementation of the Apriori algorithm in Python.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations, filterfalse\n",
    "\n",
    "#NOTE: generates all possible subsets, then we use filterfalse on \"old_association_rules\" to remove empty sets\n",
    "# this was unnecessary\n",
    "def old_powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s) + 1))\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s) + 1))\n",
    "\n",
    "###############################\n",
    "\n",
    "# this does not take the order of the antecedent into account, that is to say that\n",
    "# given itemset X and Y where X[:-1] = Y[:-1] ==> its join should be: X + Y[-1:]\n",
    "# the order is thus very important\n",
    "def old_join_set(itemsets, k):\n",
    "    return set(\n",
    "        [itemset1.union(itemset2) for itemset1 in itemsets for itemset2 in itemsets if len(itemset1.union(itemset2)) == k]\n",
    "    )\n",
    "\n",
    "# this takes the order of the antecedent into account, it should namely be of this form\n",
    "# given itemset X and Y where X[:-1] = Y[:-1] ==> its join should be: X + Y[-1:]\n",
    "def join_set(itemsets, k):\n",
    "    return set(\n",
    "        [itemset1.union(itemset2) for itemset1 in itemsets for itemset2 in itemsets if len(itemset1.union(itemset2)) == k and list(itemset1)[:-1] == list(itemset2)[:-1]]\n",
    "    )\n",
    "\n",
    "###############################\n",
    "\n",
    "def old_itemsets_support(transactions, itemsets, min_support):\n",
    "    support_count = {itemset: 0 for itemset in itemsets}\n",
    "    for transaction in transactions:\n",
    "        for itemset in itemsets:\n",
    "            if itemset.issubset(transaction):\n",
    "                support_count[itemset] += 1\n",
    "    n_transactions = len(transactions)\n",
    "    x = {itemset: support / n_transactions for itemset, support in support_count.items() if support / n_transactions >= min_support}\n",
    "    return x\n",
    "\n",
    "# NOTE: this is faster than the old version as it does not use the \"issubset\" method\n",
    "def itemsets_support(transactions, itemsets, min_support):\n",
    "    support_count = {itemset: 0 for itemset in itemsets}\n",
    "    for transaction in transactions:\n",
    "        for itemset in itemsets:\n",
    "            if itemset <= transaction:\n",
    "                support_count[itemset] += 1\n",
    "    n_transactions = len(transactions)\n",
    "    x = {itemset: support / n_transactions for itemset, support in support_count.items() if support / n_transactions >= min_support}\n",
    "    return x\n",
    "\n",
    "###############################\n",
    "\n",
    "def old_apriori(transactions, min_support):\n",
    "    items = set(chain(*transactions))\n",
    "    itemsets = [frozenset([item]) for item in items]\n",
    "    itemsets_by_length = [set()]\n",
    "    k = 1\n",
    "    while itemsets:\n",
    "        support_count = itemsets_support(transactions, itemsets, min_support)\n",
    "        itemsets_by_length.append(set(support_count.keys()))\n",
    "        k += 1\n",
    "        itemsets = join_set(itemsets, k)\n",
    "    frequent_itemsets = set(chain(*itemsets_by_length))\n",
    "    return frequent_itemsets, itemsets_by_length  \n",
    "\n",
    "\"\"\"\n",
    "In the \"old_apriori\" function, the itemsets are stored in the list \"itemsets_by_length\" where each index corresponds to the length of the itemsets. \n",
    "This allows for easy access to itemsets of a specific length. \n",
    "However, it is not necessary to store all itemsets by length as they can be generated on the fly when needed.\n",
    "\n",
    "The updated \"apriori\" function eliminates the need for storing itemsets by length. \n",
    "Instead, it directly updates the \"frequent_itemsets\" set with the frequent itemsets discovered at each iteration.\n",
    "This reduces memory usage and avoids the need for storing unnecessary intermediate results.\n",
    "\n",
    "Both versions of the function should produce the same results in terms of the frequent itemsets they return. \n",
    "The updated \"apriori\" function provides a more efficient implementation by eliminating the storage of itemsets by length.\n",
    "\"\"\"\n",
    "def apriori(transactions, min_support):\n",
    "    items = set(chain(*transactions))\n",
    "    itemsets = [frozenset([item]) for item in items]\n",
    "    frequent_itemsets = set()\n",
    "    k = 1\n",
    "    while itemsets:\n",
    "        support_count = itemsets_support(transactions, itemsets, min_support)\n",
    "        frequent_itemsets.update(support_count.keys())\n",
    "        k += 1\n",
    "        itemsets = join_set(support_count.keys(), k)\n",
    "    return frequent_itemsets\n",
    "\n",
    "###############################\n",
    "\n",
    "def old_association_rules(transactions, min_support, min_confidence):\n",
    "    frequent_itemsets = apriori(transactions, min_support)\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        for subset in filterfalse(lambda x: not x, powerset(itemset)): \n",
    "            antecedent = frozenset(subset)\n",
    "            consequent = itemset - antecedent\n",
    "            support_antecedent = len([t for t in transactions if antecedent.issubset(t)]) / len(transactions)\n",
    "            support_itemset = len([t for t in transactions if itemset.issubset(t)]) / len(transactions)\n",
    "            # calculate how many times the \n",
    "            confidence = support_itemset / support_antecedent\n",
    "            if confidence >= min_confidence:\n",
    "                rules.append((antecedent, consequent, support_itemset, confidence))\n",
    "    return rules\n",
    "\n",
    "# remove filterfalse as we did not generate empty sets\n",
    "def association_rules(transactions, min_support, min_confidence):\n",
    "    frequent_itemsets = apriori(transactions, min_support)\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        for subset in powerset(itemset): # NOTE: Not necessary to run filterfalse as I did not generate empty sets\n",
    "            antecedent = frozenset(subset)\n",
    "            consequent = itemset - antecedent\n",
    "            if consequent:  #NOTE: Skip rules with empty consequent\n",
    "                support_antecedent = len([t for t in transactions if antecedent <= t]) / len(transactions)\n",
    "                support_itemset = len([t for t in transactions if itemset <= t]) / len(transactions)\n",
    "                confidence = support_itemset / support_antecedent\n",
    "                if confidence >= min_confidence:\n",
    "                    rules.append((antecedent, consequent, support_itemset, confidence))\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'C'}) => frozenset({'B'}) (support=0.50, confidence=1.00)\n",
      "frozenset({'B'}) => frozenset({'C'}) (support=0.50, confidence=0.50)\n",
      "frozenset({'B'}) => frozenset({'A'}) (support=0.50, confidence=0.50)\n",
      "frozenset({'A'}) => frozenset({'B'}) (support=0.50, confidence=1.00)\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "transactions = [\n",
    "    {\"A\", \"B\", \"C\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"B\", \"C\"},\n",
    "    {\"B\"},\n",
    "    {\"C\",\"B\"},\n",
    "]\n",
    "\n",
    "min_support = 0.2\n",
    "min_confidence = 0.5\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "\n",
    "if not rules:\n",
    "    print(\"[]\")\n",
    "\n",
    "for antecedent, consequent, support, confidence in rules:\n",
    "    print(f\"{antecedent} => {consequent} (support={support:.2f}, confidence={confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'C'}) => frozenset({'B'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'B'}) => frozenset({'C'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'B'}) => frozenset({'A'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'A'}) => frozenset({'B'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'C'}) => frozenset({'A'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'A'}) => frozenset({'C'}) (support=0.29, confidence=0.50)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "transactions = [\n",
    "    {\"A\", \"B\", \"C\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"C\"},\n",
    "    {\"A\"},\n",
    "    {\"B\", \"C\"},\n",
    "    {\"B\"},\n",
    "    {\"C\"},\n",
    "]\n",
    "min_support = 0.2  # only items that appeared min_support times will be considered\n",
    "min_confidence = 0.5  # value between 0 and 1, 1 meaning that only rules with 100% confidence will be returned\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "for antecedent, consequent, support, confidence in rules:\n",
    "    print(f\"{antecedent} => {consequent} (support={support:.2f}, confidence={confidence:.2f})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we first define helper functions like `powerset`, `join_set`, and `itemsets_support` for finding power sets, joining itemsets, and calculating support values. Then, we implement the `apriori` function, which uses the Apriori algorithm to find frequent itemsets. Finally, we create the `association_rules` function to find the rules with minimum support and confidence.  \n",
    "\n",
    "<span style=\"color:Red\"> **TODO: TASK 1/3** </span> \n",
    "I am already impressed, although the algorithm isn't perfect. Please find and report the mistakes in the implementation, or tell me how it can be improved to correctly implement the Apriori algorithm (Task 1). \n",
    "- According to Bart, `support` and `confidence` are too high"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============================================================================\n",
    "## Next, I asked:\"How can I use association rules to build a recommendation algorithm? \", and it responded the following:  \n",
    "Association rules can be used to build a simple recommendation algorithm by analyzing the relationships between items in a dataset, such as user-item interactions or item co-occurrences in transactions. By applying the Apriori algorithm or another association rule mining algorithm, you can extract rules with high support and confidence, indicating strong associations between items. These rules can then be used to generate recommendations.\n",
    "Here's a step-by-step guide to building a recommendation algorithm using association rules:\n",
    "1. **Prepare the dataset:** Organize your data into transactions or sets of items that are typically grouped together, such as items purchased together in a shopping cart or movies watched by a user.\n",
    "2. **Apply association rule mining:** Use an algorithm like Apriori to extract frequent itemsets and generate association rules with high support and confidence. These rules represent patterns in the dataset that suggest items are frequently associated with each other.\n",
    "3. **Build a recommendation function:** Create a function that takes a set of items as input, such as the items in a user's shopping cart or a user's watched movie list. `Use` the `association rules` to `find items` that are `strongly associated` `with` the `input items` `but not already in the input set`. `Sort` the `recommended items by` `confidence`, `support`, `or another metric`, and `return` the `top N items` as recommendations.\n",
    "Here's a simple example of a recommendation function using the previously implemented Apriori algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items(input_items, rules, recommendation_type: str=\"average\", top_n=5):\n",
    "    # Initialize an empty dictionary to store the recommendations\n",
    "    recommendations = {}\n",
    "\n",
    "    # Iterate over each rule in the rules list\n",
    "    for antecedent, consequent, support, confidence in rules:\n",
    "        # Check if the antecedent is a subset of input_items and the consequent is not a subset of input_items\n",
    "        if antecedent.issubset(input_items) and not consequent.issubset(input_items):\n",
    "            # Iterate over each item in the consequent\n",
    "            for item in consequent:\n",
    "                # Check if the item is not already in the input_items\n",
    "                if item not in input_items:\n",
    "                    # If the item is not in recommendations, create an empty list for it\n",
    "                    if item not in recommendations:\n",
    "                        recommendations[item] = []\n",
    "                    # Append the confidence and support values of the rule to the item's list in recommendations\n",
    "                    recommendations[item].append((confidence, support))\n",
    "\n",
    "    # METHOD 1: Calculate the average confidence and support for each recommended item\n",
    "    def average_recommendations():\n",
    "        avg_recommendations = {\n",
    "            item: (sum(conf for conf, _ in item_rules) / len(item_rules),\n",
    "                    sum(sup for _, sup in item_rules) / len(item_rules))\n",
    "            for item, item_rules in recommendations.items()\n",
    "        }\n",
    "        return avg_recommendations\n",
    "\n",
    "    # METHOD 2: Use maximum confidence as the recommendation score\n",
    "    def max_confidence_recommendations():\n",
    "        max_conf_recommendations = {\n",
    "            item: max(scores, key=lambda x: x[0])\n",
    "            for item, scores in recommendations.items()\n",
    "        }\n",
    "        return max_conf_recommendations\n",
    "\n",
    "    # METHOD 3: Calculate lift for each recommended item\n",
    "    def lift_recommendations():\n",
    "        lift_recommendations = {\n",
    "            item: max([(confidence / support, support) for confidence, support in item_rules])\n",
    "            for item, item_rules in recommendations.items()\n",
    "        }\n",
    "        return lift_recommendations\n",
    "\n",
    "    # METHOD 4: Calculate weighted average for each recommended item\n",
    "    def weighted_average_recommendations():\n",
    "        weighted_avg_recommendations = {\n",
    "            item: (sum([(conf * sup) for conf, sup in item_rules]) / sum([sup for _, sup in item_rules]),\n",
    "                    sum([sup for _, sup in item_rules]) / len(item_rules))\n",
    "            for item, item_rules in recommendations.items()\n",
    "        }\n",
    "        return weighted_avg_recommendations\n",
    "    \n",
    "    # METHOD 5: Combine multiple metrics (confidence, support, and lift) for each recommended item\n",
    "    def combine_metrics_recommendations():\n",
    "        combined_recommendations = {\n",
    "            item: (max([conf for conf, _ in item_rules]),  # Maximum confidence\n",
    "                    max([sup for _, sup in item_rules]),  # Maximum support\n",
    "                    max([(confidence / support) for confidence, support in item_rules]))  # Maximum lift\n",
    "            for item, item_rules in recommendations.items()\n",
    "        }\n",
    "        return combined_recommendations\n",
    "\n",
    "    if recommendation_type == \"average\":\n",
    "        recommendations = average_recommendations()\n",
    "    elif recommendation_type == \"max_confidence\":\n",
    "        recommendations = max_confidence_recommendations()\n",
    "    elif recommendation_type == \"lift\":\n",
    "        recommendations = lift_recommendations()\n",
    "    elif recommendation_type == \"weighted_average\":\n",
    "        recommendations = weighted_average_recommendations()\n",
    "    elif recommendation_type == \"combine_metrics\":\n",
    "        recommendations = combine_metrics_recommendations()\n",
    "\n",
    "    # Sort the recommendations based on the chosen method in descending order\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: (-x[1][0], -x[1][1]))\n",
    "\n",
    "    # Return the top n recommended items\n",
    "    return [item for item, _ in sorted_recommendations[:top_n]]\n",
    "\n",
    "def run_recommendations(input_items, rules):\n",
    "    for recommendation_type in [\"average\", \"max_confidence\", \"lift\", \"weighted_average\", \"combine_metrics\"]:\n",
    "        print(f\"Recommendation type: {recommendation_type}\")\n",
    "        recommended_items = recommend_items(input_items, rules, recommendation_type)\n",
    "        print(f\"Recommended items: {recommended_items}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation type: average\n",
      "Recommended items: ['F', 'C']\n",
      "\n",
      "Recommendation type: max_confidence\n",
      "Recommended items: ['C', 'F']\n",
      "\n",
      "Recommendation type: lift\n",
      "Recommended items: ['C', 'F']\n",
      "\n",
      "Recommendation type: weighted_average\n",
      "Recommended items: ['F', 'C']\n",
      "\n",
      "Recommendation type: combine_metrics\n",
      "Recommended items: ['C', 'F']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_items = {\"A\", \"B\"}\n",
    "transactions = [\n",
    "    {\"A\", \"B\", \"C\"},\n",
    "    {\"A\", \"C\"},\n",
    "    {\"A\", \"G\", \"E\"},\n",
    "    {\"B\", \"C\", \"F\"},\n",
    "    {\"B\", \"A\", \"F\"},\n",
    "    {\"C\", \"D\"},\n",
    "]\n",
    "\n",
    "min_support = 0.2\n",
    "min_confidence = 0.5\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "\n",
    "run_recommendations(input_items, rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation type: average\n",
      "Recommended items: ['C']\n",
      "\n",
      "Recommendation type: max_confidence\n",
      "Recommended items: ['C']\n",
      "\n",
      "Recommendation type: lift\n",
      "Recommended items: ['C']\n",
      "\n",
      "Recommendation type: weighted_average\n",
      "Recommended items: ['C']\n",
      "\n",
      "Recommendation type: combine_metrics\n",
      "Recommended items: ['C']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here's how to use the `recommend_items` function:\n",
    "input_items = {\"A\", \"B\"}\n",
    "transactions = [\n",
    "    {\"A\", \"B\", \"C\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"C\"},\n",
    "    {\"A\"},\n",
    "    {\"B\", \"C\"},\n",
    "    {\"B\"},\n",
    "    {\"C\"},\n",
    "]\n",
    "min_support = 0.2\n",
    "min_confidence = 0.5\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "recommended_items = recommend_items(input_items, rules)\n",
    "\n",
    "run_recommendations(input_items, rules)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, given the input items {\"A\", \"B\"}, the recommendation function returns item \"C\" as a recommendation, since it has a strong association with items \"A\" and \"B\" based on the generated association rules.\n",
    "Keep in mind that this is a simple recommendation algorithm, and more sophisticated methods such as collaborative filtering, matrix factorization, or deep learning models might yield better results depending on your dataset and use case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I asked to explain the rationale behind the recommend_items function, and recieved the following response:\n",
    "Certainly! In the `recommend_items` function, we rank the items based on their association with the input items (antecedents). To do this, we calculate an aggregate score for each item using the confidence and support values of the association rules in which the item is present as a consequent. Here's a step-by-step breakdown of the ranking process:\n",
    "1. **Initialize an empty dictionary for recommendations:** We start by creating an empty dictionary called `recommendations`. The keys in this dictionary will be the items we recommend, and the values will be lists of tuples containing the confidence and support values of the association rules in which the item is a consequent.\n",
    "2. **Iterate through the association rules:** For each rule `(antecedent, consequent, support, confidence)`, we check if the `antecedent` is a `subset` ``of`` the `input items` and if the `consequent` is `not a subset` ``of`` the `input items`. This condition ensures that we only consider rules where the antecedents are present in the input items, and the consequents are not already included in the input items.\n",
    "3. **Update the recommendations dictionary:** `If` the `condition` in step 2 is `met`, we `iterate` `through` the `items` `in` the `consequent`. `For` each `item` that is `not` already `in` the `input items`, we `add` an `entry` `to` the `recommendations` dictionary with the `item as` the `key` and the rule's `confidence` `and` `support` values `as` a tuple in the corresponding `value` list.\n",
    "4. **Calculate the average confidence and support for each item:** We `create` a new `dictionary` that `contains` the `average confidence and support` for each recommended item by `summing` the `confidence and support` values of the corresponding rules and dividing by the number of rules in which the item appears as a consequent.\n",
    "5. **Sort the recommended items:** We `sort` the recommended items `based` on their `average confidence and support` values in descending order. We `first` sort `by confidence`, and `then by support` as a tie-breaker. This is done using the `sorted` function with a custom sorting key.\n",
    "6. **Return the top N recommended items:** We return the top N recommended items based on the sorted order. In this case, N is defined by the `top_n` parameter.\n",
    "In summary, the `recommend_items` function ranks the items based on the average confidence and support values of the association rules in which the items appear as consequents. This ranking approach assumes that items with higher confidence and support values are more likely to be relevant and interesting to the user, given the input items.\n",
    "\n",
    "## I also asked it to write an evaluation function:\n",
    "To evaluate the quality of the `recommend_items` function, you can use measures like precision, recall, and F1 score. To compute these metrics, you'll need a test dataset with known user-item interactions. The evaluation function would calculate how well the recommendations generated by the `recommend_items` function match the known interactions in the test dataset.\n",
    "Here's an example of an evaluation function that computes precision, recall, and F1 score for the `recommend_items` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To use this evaluation function, you'll need a test dataset in the form of a dictionary where the keys represent user IDs, and the values are sets of true items for those users. You'll also need a dictionary called `user_items` that contains the input items for each user.\n",
    "Please note that this evaluation function assumes the availability of ground truth data and that the recommendations generated by the `recommend_items` function are binary (i.e., an item is either relevant or not relevant). In practice, you may need to adapt the evaluation function to better suit your specific dataset and use case.\n",
    "\"\"\"\n",
    "def evaluate_recommendations(test_data, rules, top_n=5, user_items=None):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for user, true_items in test_data.items():\n",
    "        # Assuming user_items is a dictionary with user IDs as keys and their associated items as values\n",
    "        input_items = user_items[user]\n",
    "        # Get recommendations for the user\n",
    "        recommended_items = set(recommend_items(input_items, rules, top_n=top_n))\n",
    "        true_items = set(true_items)\n",
    "        true_positives += len(recommended_items.intersection(true_items))\n",
    "        false_positives += len(recommended_items - true_items)\n",
    "        false_negatives += len(true_items - recommended_items)\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.42857142857142855\n",
      "Recall: 0.21428571428571427\n",
      "F1 score: 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "# generate test data\n",
    "test_data = {\n",
    "    \"user1\": {\"A\", \"B\", \"C\"},\n",
    "    \"user2\": {\"A\", \"B\", \"D\"},\n",
    "    \"user3\": {\"A\", \"B\"},\n",
    "    \"user4\": {\"A\", \"C\"},\n",
    "    \"user5\": {\"A\", \"C\"},\n",
    "    \"user6\": {\"A\"},\n",
    "    \"user7\": {\"A\"},\n",
    "}\n",
    "# generate user items\n",
    "user_items = {\n",
    "    \"user1\": {\"A\", \"B\"},\n",
    "    \"user2\": {\"A\", \"B\"},\n",
    "    \"user3\": {\"A\", \"B\"},\n",
    "    \"user4\": {\"A\", \"B\"},\n",
    "    \"user5\": {\"A\", \"B\"},\n",
    "    \"user6\": {\"A\", \"B\"},\n",
    "    \"user7\": {\"A\", \"B\"},\n",
    "}\n",
    "\n",
    "# generate rules\n",
    "transactions = [\n",
    "    {\"A\", \"B\", \"C\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"C\"},\n",
    "    {\"A\"},\n",
    "    {\"B\", \"C\"},\n",
    "    {\"B\"},\n",
    "    {\"C\"},\n",
    "]\n",
    "min_support = 0.2\n",
    "min_confidence = 0.5\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "\n",
    "# evaluate recommendations\n",
    "precision, recall, f1_score = evaluate_recommendations(test_data, rules, user_items=user_items)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1 score: 0\n"
     ]
    }
   ],
   "source": [
    "# generate test data\n",
    "test_data = {\n",
    "    \"user1\": {\"A\", \"B\", \"C\"},\n",
    "    \"user2\": {\"A\", \"B\", \"D\"},\n",
    "    \"user3\": {\"A\", \"B\"},\n",
    "    \"user4\": {\"A\", \"C\"},\n",
    "    \"user5\": {\"A\", \"C\"},\n",
    "    \"user6\": {\"A\"},\n",
    "    \"user7\": {\"A\"},\n",
    "}\n",
    "# generate user items\n",
    "user_items = {\n",
    "    \"user1\": {\"A\", \"B\"},\n",
    "    \"user2\": {\"A\", \"B\"},\n",
    "    \"user3\": {\"A\", \"B\"},\n",
    "    \"user4\": {\"A\", \"B\"},\n",
    "    \"user5\": {\"A\", \"B\"},\n",
    "    \"user6\": {\"A\", \"B\"},\n",
    "    \"user7\": {\"A\", \"B\"},\n",
    "}\n",
    "\n",
    "# generate rules\n",
    "transactions = [\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"B\"},\n",
    "]\n",
    "min_support = 0.2\n",
    "min_confidence = 0.5\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "\n",
    "# evaluate recommendations\n",
    "precision, recall, f1_score = evaluate_recommendations(test_data, rules, user_items=user_items)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red\"> **TODO: TASK 2/3** </span> \n",
    "I would like you to play around with this implementation and see if you can improve it already by using different ranking methods (in stead of average confidence). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red\"> **TODO: TASK 3/3** </span> \n",
    "Last but not least, I would like you to use my implementation of Non-Derivable-Itemsets (http://www.adrem.ua.ac.be/~goethals/software/files/ndi.tgz) and check whether it generates better recommendations when only using association rules made of non-derivable itemsets (this will allow much smaller support thresholds!).  \n",
    "(Unfortunately, GPT4 could not implement NDI)\n",
    "- Please also experiment and report your results for different confidence thresholds.\n",
    "- For experiments, you can use the retail dataset here: http://fimi.uantwerpen.be/data/ or any other dataset that you can find online for the evaluation of recommender systems. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"preprocessing step that: \n",
    "--removes the first row of the dataset\n",
    "--removes the last column of the dataset\n",
    "--removes the parentheses surrounding the last column\n",
    "--finally we only consider the 1000 rows of the dataset with row interval skips of 600, this is to make sure we get a good mix of items in the dataset\n",
    "and not only get those containing only 1 item\n",
    "\n",
    "dataset here is \"out_retail510.txt\"\n",
    "the result will be written to \"processed_retail510.txt\n",
    "\"\"\"\n",
    "def preprocess_dataset(input_file, output_file, num_rows=1000):\n",
    "    with open(input_file, \"r\") as f_in:\n",
    "        with open(output_file, \"w\") as f_out:\n",
    "            # Skip the first row\n",
    "            next(f_in)\n",
    "            # remove the last column and the parentheses around the new last column\n",
    "            # read only every 800th row of the dataset and consider only 1000 entries\n",
    "            for i, line in enumerate(f_in):\n",
    "                if i % 600 == 0 and i < num_rows * 600:\n",
    "                    columns = line.split()\n",
    "                    # remove the last column\n",
    "                    modified_columns = columns[:-1]\n",
    "                    # remove the parentheses surrounding the last column\n",
    "                    modified_columns[-1] = modified_columns[-1][1:-1]\n",
    "                    # write the modified line to the output file\n",
    "                    if i+1 == num_rows * 600:\n",
    "                        f_out.write(\" \".join(modified_columns))\n",
    "                        break\n",
    "                    else:\n",
    "                        f_out.write(\" \".join(modified_columns) + \"\\n\")\n",
    "\n",
    "preprocess_dataset(\"out_retail510.txt\", \"processed_retail510.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "(frozenset({'479'}), frozenset({'41', '39'}), 0.01233974358974359, 0.927710843373494)\n",
      "(frozenset({'41', '479'}), frozenset({'39'}), 0.01233974358974359, 1.0)\n",
      "(frozenset({'41', '39'}), frozenset({'479'}), 0.01233974358974359, 0.24444444444444444)\n",
      "(frozenset({'479', '39'}), frozenset({'41'}), 0.01233974358974359, 0.9685534591194969)\n",
      "(frozenset({'89'}), frozenset({'237'}), 0.013942307692307693, 0.3365570599613153)\n",
      "(frozenset({'237'}), frozenset({'89'}), 0.013942307692307693, 0.44274809160305345)\n",
      "(frozenset({'38'}), frozenset({'48'}), 0.02123397435897436, 0.23745519713261648)\n",
      "(frozenset({'3904'}), frozenset({'48', '39'}), 0.014342948717948717, 1.0)\n",
      "(frozenset({'3904', '48'}), frozenset({'39'}), 0.014342948717948717, 1.0)\n",
      "(frozenset({'3904', '39'}), frozenset({'48'}), 0.014342948717948717, 1.0)\n",
      "(frozenset({'48', '39'}), frozenset({'3904'}), 0.014342948717948717, 0.12185159972770591)\n",
      "(frozenset({'976'}), frozenset({'48', '41'}), 0.010256410256410256, 0.9275362318840579)\n",
      "(frozenset({'48', '976'}), frozenset({'41'}), 0.010256410256410256, 0.9624060150375939)\n",
      "(frozenset({'48', '41'}), frozenset({'976'}), 0.010256410256410256, 0.22695035460992907)\n",
      "(frozenset({'976', '41'}), frozenset({'48'}), 0.010256410256410256, 1.0)\n",
      "(frozenset({'310'}), frozenset({'48'}), 0.010096153846153847, 0.5943396226415095)\n",
      "(frozenset({'1280'}), frozenset({'39'}), 0.013541666666666667, 0.9657142857142857)\n",
      "(frozenset({'32'}), frozenset({'39'}), 0.01842948717948718, 0.17584097859327216)\n",
      "(frozenset({'89'}), frozenset({'48'}), 0.026041666666666668, 0.6286266924564797)\n",
      "(frozenset({'41'}), frozenset({'38'}), 0.014423076923076924, 0.11680726800778715)\n",
      "(frozenset({'38'}), frozenset({'41'}), 0.014423076923076924, 0.16129032258064516)\n",
      "(frozenset({'1344'}), frozenset({'39'}), 0.02564102564102564, 0.9785932721712538)\n",
      "(frozenset({'1344'}), frozenset({'48'}), 0.025721153846153845, 0.9816513761467889)\n",
      "(frozenset({'48'}), frozenset({'41'}), 0.04519230769230769, 0.146455466112698)\n",
      "(frozenset({'41'}), frozenset({'48'}), 0.04519230769230769, 0.3659961064243997)\n",
      "(frozenset({'976'}), frozenset({'48'}), 0.010657051282051282, 0.9637681159420289)\n",
      "(frozenset({'32'}), frozenset({'48'}), 0.01217948717948718, 0.1162079510703364)\n",
      "(frozenset({'89'}), frozenset({'39'}), 0.012900641025641025, 0.3114119922630561)\n",
      "(frozenset({'3904'}), frozenset({'39'}), 0.014342948717948717, 1.0)\n",
      "(frozenset({'89'}), frozenset({'48', '237'}), 0.013942307692307693, 0.3365570599613153)\n",
      "(frozenset({'237'}), frozenset({'89', '48'}), 0.013942307692307693, 0.44274809160305345)\n",
      "(frozenset({'89', '48'}), frozenset({'237'}), 0.013942307692307693, 0.5353846153846153)\n",
      "(frozenset({'89', '237'}), frozenset({'48'}), 0.013942307692307693, 1.0)\n",
      "(frozenset({'48', '237'}), frozenset({'89'}), 0.013942307692307693, 0.7532467532467533)\n",
      "(frozenset({'479'}), frozenset({'39'}), 0.012740384615384615, 0.9578313253012049)\n",
      "(frozenset({'48'}), frozenset({'39'}), 0.11770833333333333, 0.38145936120488183)\n",
      "(frozenset({'39'}), frozenset({'48'}), 0.11770833333333333, 0.37237008871989863)\n",
      "(frozenset({'38'}), frozenset({'39'}), 0.03205128205128205, 0.3584229390681003)\n",
      "(frozenset({'39'}), frozenset({'38'}), 0.03205128205128205, 0.10139416983523447)\n",
      "(frozenset({'589'}), frozenset({'39'}), 0.010737179487179487, 0.783625730994152)\n",
      "(frozenset({'2492'}), frozenset({'39'}), 0.017387820512820513, 1.0)\n",
      "(frozenset({'976'}), frozenset({'41'}), 0.010256410256410256, 0.9275362318840579)\n",
      "(frozenset({'3904'}), frozenset({'48'}), 0.014342948717948717, 1.0)\n",
      "(frozenset({'479'}), frozenset({'41'}), 0.01233974358974359, 0.927710843373494)\n",
      "(frozenset({'48', '41'}), frozenset({'39'}), 0.01233974358974359, 0.2730496453900709)\n",
      "(frozenset({'48', '39'}), frozenset({'41'}), 0.01233974358974359, 0.10483321987746766)\n",
      "(frozenset({'41', '39'}), frozenset({'48'}), 0.01233974358974359, 0.24444444444444444)\n",
      "(frozenset({'1344'}), frozenset({'48', '39'}), 0.02516025641025641, 0.9602446483180428)\n",
      "(frozenset({'48', '1344'}), frozenset({'39'}), 0.02516025641025641, 0.9781931464174456)\n",
      "(frozenset({'48', '39'}), frozenset({'1344'}), 0.02516025641025641, 0.2137508509189925)\n",
      "(frozenset({'1344', '39'}), frozenset({'48'}), 0.02516025641025641, 0.9812500000000001)\n",
      "(frozenset({'41'}), frozenset({'39'}), 0.05048076923076923, 0.408825438027255)\n",
      "(frozenset({'39'}), frozenset({'41'}), 0.05048076923076923, 0.1596958174904943)\n",
      "(frozenset({'548'}), frozenset({'32'}), 0.022435897435897436, 0.8832807570977919)\n",
      "(frozenset({'32'}), frozenset({'548'}), 0.022435897435897436, 0.21406727828746178)\n",
      "(frozenset({'237'}), frozenset({'48'}), 0.018509615384615385, 0.5877862595419847)\n"
     ]
    }
   ],
   "source": [
    "# read the processed dataset (here \"processed_retail510.txt\")\n",
    "# the first columns excluding the last one are the item(s) which were bought together\n",
    "# the last column is the number of times the item(s) were bought together\n",
    "\n",
    "# the goal is to initialize the \"transaction\" variable in the form of: transaction = [{item1, item2, ...}, {item1, item2, ...}, ...]\n",
    "# where the amount of times for example the transaction {item1, item2, ...} appears in the dataset is equal to the number of times the item(s) were bought together (the last column)\n",
    "\n",
    "transactions = []\n",
    "with open(\"processed_retail510.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        columns = line.split()\n",
    "        # Get the items in the transaction\n",
    "        items = set(columns[:-1])\n",
    "        # Get the number of times the items were bought together\n",
    "        count = int(columns[-1])\n",
    "        # Add the items to the list of transactions\n",
    "        transactions.extend([items] * count)\n",
    "\n",
    "# generate rules\n",
    "min_support = 0.01\n",
    "min_confidence = 0.10\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "\n",
    "# print the rules\n",
    "print(len(rules))\n",
    "for rule in rules:\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_confidence: 0.05--> 70 rules\n",
      "min_confidence: 0.1--> 56 rules\n",
      "min_confidence: 0.15000000000000002--> 50 rules\n",
      "min_confidence: 0.2--> 47 rules\n",
      "min_confidence: 0.25--> 41 rules\n",
      "min_confidence: 0.3--> 40 rules\n",
      "min_confidence: 0.35--> 37 rules\n",
      "min_confidence: 0.39999999999999997--> 33 rules\n",
      "min_confidence: 0.44999999999999996--> 30 rules\n",
      "min_confidence: 0.49999999999999994--> 30 rules\n",
      "min_confidence: 0.5499999999999999--> 29 rules\n",
      "min_confidence: 0.6--> 27 rules\n",
      "min_confidence: 0.65--> 26 rules\n",
      "min_confidence: 0.7000000000000001--> 26 rules\n",
      "min_confidence: 0.7500000000000001--> 26 rules\n",
      "min_confidence: 0.8000000000000002--> 24 rules\n",
      "min_confidence: 0.8500000000000002--> 24 rules\n",
      "min_confidence: 0.9000000000000002--> 23 rules\n",
      "min_confidence: 0.9500000000000003--> 19 rules\n"
     ]
    }
   ],
   "source": [
    "# with a constant min_support of 0.01 and a varying min_confidence with intervals of 0.05 from 0.05 to 0.95\n",
    "min_support = 0.01\n",
    "min_confidence = 0.05\n",
    "while min_confidence < 1:\n",
    "    # generate rules\n",
    "    rules = association_rules(transactions, min_support, min_confidence)\n",
    "    print(f\"min_confidence: {min_confidence}--> {len(rules)} rules\")\n",
    "    min_confidence += 0.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO SUBMIT:\n",
    "- documented code and a script to run your tests (data included)\n",
    "- a paper (PDF) in which you describe your findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

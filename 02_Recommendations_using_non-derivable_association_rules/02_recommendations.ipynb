{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last weekend, I asked GPT4 from OpenAI to implement an association rule mining algorithm in Python. GPT-4 responded with the following:\n",
    "`Association rule mining is a technique used in data mining to discover relationships between variables in large datasets. One popular algorithm for association rule mining is the Apriori algorithm. The following is a simple implementation of the Apriori algorithm in Python.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations, filterfalse\n",
    "\n",
    "# generates all possible subsets, then we use filterfalse\n",
    "def old_powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s) + 1))\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s) + 1))\n",
    "\n",
    "\n",
    "\n",
    "def join_set(itemsets, k):\n",
    "    return set(\n",
    "        [itemset1.union(itemset2) for itemset1 in itemsets for itemset2 in itemsets if len(itemset1.union(itemset2)) == k]\n",
    "    )\n",
    "###############################\n",
    "def old_itemsets_support(transactions, itemsets, min_support):\n",
    "    support_count = {itemset: 0 for itemset in itemsets}\n",
    "    for transaction in transactions:\n",
    "        for itemset in itemsets:\n",
    "            if itemset.issubset(transaction):\n",
    "                support_count[itemset] += 1\n",
    "    n_transactions = len(transactions)\n",
    "    x = {itemset: support / n_transactions for itemset, support in support_count.items() if support / n_transactions >= min_support}\n",
    "    return x\n",
    "\n",
    "def itemsets_support(transactions, itemsets, min_support):\n",
    "    support_count = {itemset: 0 for itemset in itemsets}\n",
    "    for transaction in transactions:\n",
    "        for itemset in itemsets:\n",
    "            if itemset <= transaction:  # Does the same as itemset.issubset(transaction) but faster\n",
    "                support_count[itemset] += 1\n",
    "    n_transactions = len(transactions)\n",
    "    x = {itemset: support / n_transactions for itemset, support in support_count.items() if support / n_transactions >= min_support}\n",
    "    return x\n",
    "###############################\n",
    "\n",
    "def old_apriori(transactions, min_support):\n",
    "    items = set(chain(*transactions))\n",
    "    itemsets = [frozenset([item]) for item in items]\n",
    "    itemsets_by_length = [set()]\n",
    "    k = 1\n",
    "    while itemsets:\n",
    "        support_count = itemsets_support(transactions, itemsets, min_support)\n",
    "        itemsets_by_length.append(set(support_count.keys()))\n",
    "        k += 1\n",
    "        itemsets = join_set(itemsets, k)\n",
    "    frequent_itemsets = set(chain(*itemsets_by_length))\n",
    "    return frequent_itemsets, itemsets_by_length  \n",
    "\n",
    "# no need to track itemsets by length\n",
    "def apriori(transactions, min_support):\n",
    "    items = set(chain(*transactions))\n",
    "    itemsets = [frozenset([item]) for item in items]\n",
    "    frequent_itemsets = set()\n",
    "    k = 1\n",
    "    while itemsets:\n",
    "        support_count = itemsets_support(transactions, itemsets, min_support)\n",
    "        frequent_itemsets.update(support_count.keys())\n",
    "        k += 1\n",
    "        itemsets = join_set(support_count.keys(), k)\n",
    "    return frequent_itemsets\n",
    "\n",
    "###############################\n",
    "\n",
    "def old_association_rules(transactions, min_support, min_confidence):\n",
    "    frequent_itemsets = apriori(transactions, min_support)\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        for subset in filterfalse(lambda x: not x, powerset(itemset)): \n",
    "            antecedent = frozenset(subset)\n",
    "            consequent = itemset - antecedent\n",
    "            support_antecedent = len([t for t in transactions if antecedent.issubset(t)]) / len(transactions)\n",
    "            support_itemset = len([t for t in transactions if itemset.issubset(t)]) / len(transactions)\n",
    "            # calculate how many times the \n",
    "            confidence = support_itemset / support_antecedent\n",
    "            if confidence >= min_confidence:\n",
    "                rules.append((antecedent, consequent, support_itemset, confidence))\n",
    "    return rules\n",
    "\n",
    "# remove filterfalse as we did not generate empty sets\n",
    "def association_rules(transactions, min_support, min_confidence):\n",
    "    frequent_itemsets = apriori(transactions, min_support)\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        for subset in powerset(itemset):\n",
    "            antecedent = frozenset(subset)\n",
    "            consequent = itemset - antecedent\n",
    "            if consequent:  # Skip rules with empty consequent\n",
    "                support_antecedent = len([t for t in transactions if antecedent <= t]) / len(transactions)\n",
    "                support_itemset = len([t for t in transactions if itemset <= t]) / len(transactions)\n",
    "                confidence = support_itemset / support_antecedent\n",
    "                if confidence >= min_confidence:\n",
    "                    rules.append((antecedent, consequent, support_itemset, confidence))\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'B'}) => frozenset({'C'}) (support=0.50, confidence=0.50)\n",
      "frozenset({'C'}) => frozenset({'B'}) (support=0.50, confidence=1.00)\n",
      "frozenset({'A'}) => frozenset({'B'}) (support=0.50, confidence=1.00)\n",
      "frozenset({'B'}) => frozenset({'A'}) (support=0.50, confidence=0.50)\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "transactions = [\n",
    "    {\"A\", \"B\", \"C\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"B\", \"C\"},\n",
    "    {\"B\"},\n",
    "    {\"C\",\"B\"},\n",
    "]\n",
    "\n",
    "min_support = 0.2\n",
    "min_confidence = 0.5\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "\n",
    "if not rules:\n",
    "    print(\"[]\")\n",
    "\n",
    "for antecedent, consequent, support, confidence in rules:\n",
    "    print(f\"{antecedent} => {consequent} (support={support:.2f}, confidence={confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'B'}) => frozenset({'C'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'C'}) => frozenset({'B'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'A'}) => frozenset({'C'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'C'}) => frozenset({'A'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'A'}) => frozenset({'B'}) (support=0.29, confidence=0.50)\n",
      "frozenset({'B'}) => frozenset({'A'}) (support=0.29, confidence=0.50)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "transactions = [\n",
    "    {\"A\", \"B\", \"C\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"C\"},\n",
    "    {\"A\"},\n",
    "    {\"B\", \"C\"},\n",
    "    {\"B\"},\n",
    "    {\"C\"},\n",
    "]\n",
    "min_support = 0.2  # only items that appeared min_support times will be considered\n",
    "min_confidence = 0.5  # value between 0 and 1, 1 meaning that only rules with 100% confidence will be returned\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "for antecedent, consequent, support, confidence in rules:\n",
    "    print(f\"{antecedent} => {consequent} (support={support:.2f}, confidence={confidence:.2f})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we first define helper functions like `powerset`, `join_set`, and `itemsets_support` for finding power sets, joining itemsets, and calculating support values. Then, we implement the `apriori` function, which uses the Apriori algorithm to find frequent itemsets. Finally, we create the `association_rules` function to find the rules with minimum support and confidence.  \n",
    "\n",
    "<span style=\"color:Red\"> **TODO: TASK 1/3** </span> \n",
    "I am already impressed, although the algorithm isn't perfect. Please find and report the mistakes in the implementation, or tell me how it can be improved to correctly implement the Apriori algorithm (Task 1). \n",
    "- According to Bart, `support` and `confidence` are too high"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============================================================================\n",
    "## Next, I asked:\"How can I use association rules to build a recommendation algorithm? \", and it responded the following:  \n",
    "Association rules can be used to build a simple recommendation algorithm by analyzing the relationships between items in a dataset, such as user-item interactions or item co-occurrences in transactions. By applying the Apriori algorithm or another association rule mining algorithm, you can extract rules with high support and confidence, indicating strong associations between items. These rules can then be used to generate recommendations.\n",
    "Here's a step-by-step guide to building a recommendation algorithm using association rules:\n",
    "1. **Prepare the dataset:** Organize your data into transactions or sets of items that are typically grouped together, such as items purchased together in a shopping cart or movies watched by a user.\n",
    "2. **Apply association rule mining:** Use an algorithm like Apriori to extract frequent itemsets and generate association rules with high support and confidence. These rules represent patterns in the dataset that suggest items are frequently associated with each other.\n",
    "3. **Build a recommendation function:** Create a function that takes a set of items as input, such as the items in a user's shopping cart or a user's watched movie list. `Use` the `association rules` to `find items` that are `strongly associated` `with` the `input items` `but not already in the input set`. `Sort` the `recommended items by` `confidence`, `support`, `or another metric`, and `return` the `top N items` as recommendations.\n",
    "Here's a simple example of a recommendation function using the previously implemented Apriori algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items(input_items, rules, recommendation_type: str = \"average\", top_n=5):\n",
    "    # Initialize an empty dictionary to store the recommendations\n",
    "    recommendations = {}\n",
    "\n",
    "    # Iterate over each rule in the rules list\n",
    "    for antecedent, consequent, support, confidence in rules:\n",
    "        # Check if the antecedent is a subset of input_items and the consequent is not a subset of input_items\n",
    "        if antecedent.issubset(input_items) and not consequent.issubset(input_items):\n",
    "            # Iterate over each item in the consequent\n",
    "            for item in consequent:\n",
    "                # Check if the item is not already in the input_items\n",
    "                if item not in input_items:\n",
    "                    # If the item is not in recommendations, create an empty list for it\n",
    "                    if item not in recommendations:\n",
    "                        recommendations[item] = []\n",
    "                    # Append the confidence and support values of the rule to the item's list in recommendations\n",
    "                    recommendations[item].append((confidence, support))\n",
    "\n",
    "    # METHOD 1: Calculate the average confidence and support for each recommended item\n",
    "    def average_recommendations():\n",
    "        avg_recommendations = {\n",
    "            item: (sum(conf for conf, _ in item_rules) / len(item_rules),\n",
    "                    sum(sup for _, sup in item_rules) / len(item_rules))\n",
    "            for item, item_rules in recommendations.items()\n",
    "        }\n",
    "        return avg_recommendations\n",
    "\n",
    "    # METHOD 2: Use maximum confidence as the recommendation score\n",
    "    def max_confidence_recommendations():\n",
    "        max_conf_recommendations = {\n",
    "            item: max(scores, key=lambda x: x[0])\n",
    "            for item, scores in recommendations.items()\n",
    "        }\n",
    "        return max_conf_recommendations\n",
    "\n",
    "    # METHOD 3: Calculate lift for each recommended item\n",
    "    def lift_recommendations():\n",
    "        lift_recommendations = {\n",
    "            item: max([(confidence / support, support) for confidence, support in item_rules])\n",
    "            for item, item_rules in recommendations.items()\n",
    "        }\n",
    "        return lift_recommendations\n",
    "\n",
    "    if recommendation_type == \"average\":\n",
    "        recommendations = average_recommendations()\n",
    "    elif recommendation_type == \"max_confidence\":\n",
    "        recommendations = max_confidence_recommendations()\n",
    "    elif recommendation_type == \"lift\":\n",
    "        recommendations = lift_recommendations()\n",
    "\n",
    "    # Sort the recommendations based on the chosen method in descending order\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: (-x[1][0], -x[1][1]))\n",
    "\n",
    "    # Return the top n recommended items\n",
    "    return [item for item, _ in sorted_recommendations[:top_n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items (Average): ['C']\n",
      "Recommended items (Max_conf): ['C']\n",
      "Recommended items (Lift): ['C']\n"
     ]
    }
   ],
   "source": [
    "# Here's how to use the `recommend_items` function:\n",
    "input_items = {\"A\", \"B\"}\n",
    "transactions = [\n",
    "    {\"A\", \"B\", \"C\"},\n",
    "    {\"A\", \"B\"},\n",
    "    {\"A\", \"C\"},\n",
    "    {\"A\"},\n",
    "    {\"B\", \"C\"},\n",
    "    {\"B\"},\n",
    "    {\"C\"},\n",
    "]\n",
    "min_support = 0.2\n",
    "min_confidence = 0.5\n",
    "rules = association_rules(transactions, min_support, min_confidence)\n",
    "recommended_items = recommend_items(input_items, rules)\n",
    "print(\"Recommended items (Average):\", recommended_items)\n",
    "\n",
    "recommended_items = recommend_items(input_items, rules, recommendation_type=\"max_confidence\")\n",
    "print(\"Recommended items (Max_conf):\", recommended_items)\n",
    "\n",
    "recommended_items = recommend_items(input_items, rules, recommendation_type=\"lift\")\n",
    "print(\"Recommended items (Lift):\", recommended_items)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, given the input items {\"A\", \"B\"}, the recommendation function returns item \"C\" as a recommendation, since it has a strong association with items \"A\" and \"B\" based on the generated association rules.\n",
    "Keep in mind that this is a simple recommendation algorithm, and more sophisticated methods such as collaborative filtering, matrix factorization, or deep learning models might yield better results depending on your dataset and use case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I asked to explain the rationale behind the recommend_items function, and recieved the following response:\n",
    "Certainly! In the `recommend_items` function, we rank the items based on their association with the input items (antecedents). To do this, we calculate an aggregate score for each item using the confidence and support values of the association rules in which the item is present as a consequent. Here's a step-by-step breakdown of the ranking process:\n",
    "1. **Initialize an empty dictionary for recommendations:** We start by creating an empty dictionary called `recommendations`. The keys in this dictionary will be the items we recommend, and the values will be lists of tuples containing the confidence and support values of the association rules in which the item is a consequent.\n",
    "2. **Iterate through the association rules:** For each rule `(antecedent, consequent, support, confidence)`, we check if the `antecedent` is a `subset` ``of`` the `input items` and if the `consequent` is `not a subset` ``of`` the `input items`. This condition ensures that we only consider rules where the antecedents are present in the input items, and the consequents are not already included in the input items.\n",
    "3. **Update the recommendations dictionary:** `If` the `condition` in step 2 is `met`, we `iterate` `through` the `items` `in` the `consequent`. `For` each `item` that is `not` already `in` the `input items`, we `add` an `entry` `to` the `recommendations` dictionary with the `item as` the `key` and the rule's `confidence` `and` `support` values `as` a tuple in the corresponding `value` list.\n",
    "4. **Calculate the average confidence and support for each item:** We `create` a new `dictionary` that `contains` the `average confidence and support` for each recommended item by `summing` the `confidence and support` values of the corresponding rules and dividing by the number of rules in which the item appears as a consequent.\n",
    "5. **Sort the recommended items:** We `sort` the recommended items `based` on their `average confidence and support` values in descending order. We `first` sort `by confidence`, and `then by support` as a tie-breaker. This is done using the `sorted` function with a custom sorting key.\n",
    "6. **Return the top N recommended items:** We return the top N recommended items based on the sorted order. In this case, N is defined by the `top_n` parameter.\n",
    "In summary, the `recommend_items` function ranks the items based on the average confidence and support values of the association rules in which the items appear as consequents. This ranking approach assumes that items with higher confidence and support values are more likely to be relevant and interesting to the user, given the input items.\n",
    "\n",
    "## I also asked it to write an evaluation function:\n",
    "To evaluate the quality of the `recommend_items` function, you can use measures like precision, recall, and F1 score. To compute these metrics, you'll need a test dataset with known user-item interactions. The evaluation function would calculate how well the recommendations generated by the `recommend_items` function match the known interactions in the test dataset.\n",
    "Here's an example of an evaluation function that computes precision, recall, and F1 score for the `recommend_items` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To use this evaluation function, you'll need a test dataset in the form of a dictionary where the keys represent user IDs, and the values are sets of true items for those users. You'll also need a dictionary called `user_items` that contains the input items for each user.\n",
    "Please note that this evaluation function assumes the availability of ground truth data and that the recommendations generated by the `recommend_items` function are binary (i.e., an item is either relevant or not relevant). In practice, you may need to adapt the evaluation function to better suit your specific dataset and use case.\n",
    "\"\"\"\n",
    "def evaluate_recommendations(test_data, rules, top_n=5):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for user, true_items in test_data.items():\n",
    "        # Assuming user_items is a dictionary with user IDs as keys and their associated items as values\n",
    "        input_items = user_items[user]\n",
    "        # Get recommendations for the user\n",
    "        recommended_items = set(recommend_items(input_items, rules, top_n=top_n))\n",
    "        true_items = set(true_items)\n",
    "        true_positives += len(recommended_items.intersection(true_items))\n",
    "        false_positives += len(recommended_items - true_items)\n",
    "        false_negatives += len(true_items - recommended_items)\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red\"> **TODO: TASK 2/3** </span> \n",
    "I would like you to play around with this implementation and see if you can improve it already by using different ranking methods (in stead of average confidence). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red\"> **TODO: TASK 3/3** </span> \n",
    "Last but not least, I would like you to use my implementation of Non-Derivable-Itemsets (http://www.adrem.ua.ac.be/~goethals/software/files/ndi.tgz) and check whether it generates better recommendations when only using association rules made of non-derivable itemsets (this will allow much smaller support thresholds!).  \n",
    "(Unfortunately, GPT4 could not implement NDI)\n",
    "- Please also experiment and report your results for different confidence thresholds.\n",
    "- For experiments, you can use the retail dataset here: http://fimi.uantwerpen.be/data/ or any other dataset that you can find online for the evaluation of recommender systems. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO SUBMIT:\n",
    "- documented code and a script to run your tests (data included)\n",
    "- a paper (PDF) in which you describe your findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
